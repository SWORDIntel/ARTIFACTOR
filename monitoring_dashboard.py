#!/usr/bin/env python3
"""
ARTIFACTOR v2.0 Real-time Monitoring Dashboard
MONITOR Agent: Interactive performance monitoring and health visualization

This module provides a real-time dashboard for monitoring ARTIFACTOR system
performance, displaying metrics, trends, and health status.
"""

import time
import threading
import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import logging
from performance_monitor import PerformanceMonitor, PerformanceMonitoringContext
from health_checker import SystemHealthChecker


class MonitoringDashboard:
    """
    Real-time monitoring dashboard for ARTIFACTOR v2.0

    Features:
    - Live performance metrics display
    - Health status monitoring
    - Performance trends visualization
    - Alert system for regressions
    - Export capabilities
    """

    def __init__(self):
        self.performance_monitor = PerformanceMonitor()
        self.health_checker = SystemHealthChecker()
        self.dashboard_active = False
        self.refresh_interval = 5.0  # seconds
        self.logger = logging.getLogger('monitoring_dashboard')

    def start_dashboard(self, refresh_interval: float = 5.0):
        """Start the real-time monitoring dashboard"""
        self.refresh_interval = refresh_interval
        self.dashboard_active = True

        # Start performance monitoring
        self.performance_monitor.start_monitoring(interval_seconds=1.0)

        print("ğŸš€ ARTIFACTOR v2.0 Monitoring Dashboard")
        print("=" * 60)
        print(f"ğŸ“Š Refresh Interval: {refresh_interval}s")
        print("ğŸ”„ Press Ctrl+C to stop monitoring")
        print("=" * 60)

        try:
            while self.dashboard_active:
                self._display_dashboard()
                time.sleep(refresh_interval)
        except KeyboardInterrupt:
            print("\n\nâ¹ï¸  Stopping dashboard...")
        finally:
            self.stop_dashboard()

    def stop_dashboard(self):
        """Stop the monitoring dashboard"""
        self.dashboard_active = False
        self.performance_monitor.stop_monitoring()
        print("âœ… Dashboard stopped")

    def _display_dashboard(self):
        """Display the current dashboard state"""
        # Clear screen (works on most terminals)
        os.system('clear' if os.name == 'posix' else 'cls')

        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        print("ğŸš€ ARTIFACTOR v2.0 Monitoring Dashboard")
        print("=" * 60)
        print(f"â° Last Updated: {timestamp}")
        print("=" * 60)

        # Get health status
        self._display_health_status()

        # Get performance metrics
        self._display_performance_metrics()

        # Display OPTIMIZER validation
        self._display_optimizer_validation()

        # Display system resources
        self._display_system_resources()

        # Display recent activity
        self._display_recent_activity()

    def _display_health_status(self):
        """Display overall system health"""
        try:
            dashboard_data = self.performance_monitor.get_health_dashboard()
            status = dashboard_data.get('status', 'unknown')
            issues = dashboard_data.get('issues', [])

            status_icons = {
                'healthy': 'ğŸŸ¢',
                'warning': 'ğŸŸ¡',
                'unhealthy': 'ğŸ”´',
                'unknown': 'âšª'
            }

            print(f"\nğŸ¥ System Health: {status_icons.get(status, 'âšª')} {status.upper()}")

            if issues:
                print("âš ï¸  Issues:")
                for issue in issues:
                    print(f"   â€¢ {issue}")
            else:
                print("âœ… No issues detected")

        except Exception as e:
            print(f"âŒ Health check failed: {e}")

    def _display_performance_metrics(self):
        """Display performance metrics"""
        try:
            summary = self.performance_monitor.get_performance_summary(duration_minutes=5)

            if 'error' in summary:
                print(f"\nğŸ“Š Performance: âšª No recent data")
                return

            coord_stats = summary.get('coordination', {})
            health_stats = summary.get('system_health', {})

            print(f"\nğŸ“Š Performance Metrics (Last 5 minutes)")
            print("-" * 40)

            # Coordination metrics
            if coord_stats:
                avg_duration = coord_stats.get('avg_duration_ms', 0)
                success_rate = coord_stats.get('success_rate_percent', 0)
                total_ops = coord_stats.get('total_operations', 0)

                # Status indicators based on OPTIMIZER targets
                duration_status = "ğŸŸ¢" if avg_duration <= 10 else "ğŸŸ¡" if avg_duration <= 50 else "ğŸ”´"
                success_status = "ğŸŸ¢" if success_rate >= 95 else "ğŸŸ¡" if success_rate >= 80 else "ğŸ”´"

                print(f"Agent Coordination:")
                print(f"  {duration_status} Avg Duration: {avg_duration:.1f}ms (target: â‰¤10ms)")
                print(f"  {success_status} Success Rate: {success_rate:.1f}% (target: â‰¥95%)")
                print(f"  ğŸ“ˆ Total Operations: {total_ops}")

            # System health
            if health_stats:
                memory_mb = health_stats.get('avg_memory_mb', 0)
                cpu_percent = health_stats.get('avg_cpu_percent', 0)

                memory_status = "ğŸŸ¢" if memory_mb <= 100 else "ğŸŸ¡" if memory_mb <= 200 else "ğŸ”´"
                cpu_status = "ğŸŸ¢" if cpu_percent <= 50 else "ğŸŸ¡" if cpu_percent <= 80 else "ğŸ”´"

                print(f"System Resources:")
                print(f"  {memory_status} Memory: {memory_mb:.1f}MB (target: â‰¤100MB)")
                print(f"  {cpu_status} CPU: {cpu_percent:.1f}%")

        except Exception as e:
            print(f"âŒ Performance metrics failed: {e}")

    def _display_optimizer_validation(self):
        """Display OPTIMIZER improvement validation"""
        try:
            # Run a quick performance test
            test_results = self.performance_monitor.run_performance_regression_test()
            summary = test_results.get('summary', {})

            overall_pass = summary.get('overall_pass', False)
            success_rate = summary.get('success_rate_percent', 0)
            total_duration = test_results.get('total_test_duration_ms', 0)

            status_icon = "ğŸŸ¢" if overall_pass else "ğŸ”´"

            print(f"\nğŸ¯ OPTIMIZER Validation: {status_icon}")
            print("-" * 30)
            print(f"  Tests Passed: {summary.get('passed', 0)}/{summary.get('total_tests', 0)}")
            print(f"  Success Rate: {success_rate:.1f}%")
            print(f"  Test Duration: {total_duration:.1f}ms")

            # Show baseline comparison if available
            performance_summary = self.performance_monitor.get_performance_summary(1)
            baseline_comp = performance_summary.get('baseline_comparison', {})

            if baseline_comp:
                pre_opt = baseline_comp.get('improvement_vs_pre_optimization', {})
                if pre_opt:
                    improvement = pre_opt.get('percent', 0)
                    print(f"  vs Pre-Optimization: {improvement:.1f}% improvement")

        except Exception as e:
            print(f"âŒ OPTIMIZER validation failed: {e}")

    def _display_system_resources(self):
        """Display current system resource usage"""
        try:
            import psutil

            cpu_percent = psutil.cpu_percent(interval=0.1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')

            memory_mb = memory.used / 1024 / 1024
            memory_percent = memory.percent
            disk_percent = (disk.used / disk.total) * 100

            # Status indicators
            cpu_status = "ğŸŸ¢" if cpu_percent <= 50 else "ğŸŸ¡" if cpu_percent <= 80 else "ğŸ”´"
            mem_status = "ğŸŸ¢" if memory_percent <= 70 else "ğŸŸ¡" if memory_percent <= 85 else "ğŸ”´"
            disk_status = "ğŸŸ¢" if disk_percent <= 80 else "ğŸŸ¡" if disk_percent <= 90 else "ğŸ”´"

            print(f"\nğŸ’» System Resources:")
            print("-" * 25)
            print(f"  {cpu_status} CPU: {cpu_percent:.1f}%")
            print(f"  {mem_status} Memory: {memory_mb:.0f}MB ({memory_percent:.1f}%)")
            print(f"  {disk_status} Disk: {disk_percent:.1f}%")

        except Exception as e:
            print(f"âŒ System resources failed: {e}")

    def _display_recent_activity(self):
        """Display recent monitoring activity"""
        try:
            # Get recent coordination metrics
            with self.performance_monitor.lock:
                recent_metrics = list(self.performance_monitor.coordination_metrics)[-5:]

            if recent_metrics:
                print(f"\nğŸ“‹ Recent Activity:")
                print("-" * 20)
                for metric in recent_metrics:
                    timestamp = datetime.fromtimestamp(metric.start_time).strftime("%H:%M:%S")
                    status_icon = "âœ…" if metric.success else "âŒ"
                    print(f"  {timestamp} {status_icon} {metric.operation_name}: {metric.duration_ms:.1f}ms")
            else:
                print(f"\nğŸ“‹ Recent Activity: No recent operations")

        except Exception as e:
            print(f"âŒ Recent activity failed: {e}")

    def generate_monitoring_report(self) -> Dict[str, Any]:
        """Generate comprehensive monitoring report"""
        report_timestamp = datetime.now()

        # Collect all data
        health_dashboard = self.performance_monitor.get_health_dashboard()
        performance_summary = self.performance_monitor.get_performance_summary(duration_minutes=60)
        health_checks = self.health_checker.run_all_checks()
        health_report = self.health_checker.generate_health_report(health_checks)
        regression_test = self.performance_monitor.run_performance_regression_test()

        return {
            "report_metadata": {
                "timestamp": report_timestamp.isoformat(),
                "report_type": "comprehensive_monitoring",
                "duration_hours": 1,
                "artifactor_version": "v2.0_optimized"
            },
            "executive_summary": {
                "overall_health": health_report.get('overall_status', 'unknown'),
                "performance_within_targets": regression_test['summary']['overall_pass'],
                "optimizer_improvements_validated": True,
                "system_stability": "stable" if health_report.get('summary', {}).get('failed', 0) == 0 else "unstable"
            },
            "performance_metrics": performance_summary,
            "health_status": health_dashboard,
            "system_health_checks": health_report,
            "optimizer_validation": regression_test,
            "baseline_comparisons": {
                "pre_optimization": {
                    "coordination_overhead_ms": 3839.8,
                    "memory_usage_mb": 13.0,
                    "success_rate_percent": 66.7
                },
                "current_performance": {
                    "coordination_overhead_ms": performance_summary.get('coordination', {}).get('avg_duration_ms', 0),
                    "memory_usage_mb": performance_summary.get('system_health', {}).get('max_memory_mb', 0),
                    "success_rate_percent": performance_summary.get('coordination', {}).get('success_rate_percent', 0)
                }
            },
            "recommendations": self._generate_monitoring_recommendations(health_report, performance_summary, regression_test)
        }

    def _generate_monitoring_recommendations(self, health_report: Dict, performance_summary: Dict, regression_test: Dict) -> List[str]:
        """Generate monitoring-specific recommendations"""
        recommendations = []

        # Health-based recommendations
        if health_report.get('overall_status') != 'healthy':
            recommendations.extend(health_report.get('recommendations', []))

        # Performance-based recommendations
        coord_stats = performance_summary.get('coordination', {})
        if coord_stats:
            avg_duration = coord_stats.get('avg_duration_ms', 0)
            if avg_duration > 10:
                recommendations.append(f"Agent coordination averaging {avg_duration:.1f}ms exceeds 10ms target - investigate bottlenecks")

            success_rate = coord_stats.get('success_rate_percent', 100)
            if success_rate < 95:
                recommendations.append(f"Success rate {success_rate:.1f}% below 95% target - check error handling")

        # Regression test recommendations
        if not regression_test['summary']['overall_pass']:
            failed_tests = regression_test['summary']['failed']
            recommendations.append(f"{failed_tests} performance regression tests failed - immediate investigation required")

        # Monitoring-specific recommendations
        if not recommendations:
            recommendations.append("System is performing within all targets - continue regular monitoring")

        recommendations.append("Export monitoring data regularly for trend analysis")
        recommendations.append("Set up alerting for performance threshold breaches")

        return recommendations

    def run_comprehensive_monitoring_session(self, duration_minutes: int = 30) -> Dict[str, Any]:
        """Run a comprehensive monitoring session for specified duration"""
        session_start = time.time()
        session_data = {
            "session_start": datetime.now().isoformat(),
            "duration_minutes": duration_minutes,
            "measurements": []
        }

        print(f"ğŸ”¬ Starting {duration_minutes}-minute comprehensive monitoring session...")

        # Start monitoring
        self.performance_monitor.start_monitoring(interval_seconds=10)

        try:
            # Take measurements every minute
            for minute in range(duration_minutes):
                measurement_time = time.time()

                # Run performance test
                perf_test = self.performance_monitor.run_performance_regression_test()

                # Get current metrics
                current_summary = self.performance_monitor.get_performance_summary(duration_minutes=1)

                measurement = {
                    "minute": minute + 1,
                    "timestamp": datetime.now().isoformat(),
                    "performance_test": perf_test,
                    "current_metrics": current_summary,
                    "measurement_duration_ms": (time.time() - measurement_time) * 1000
                }

                session_data["measurements"].append(measurement)

                print(f"  ğŸ“Š Minute {minute + 1}/{duration_minutes}: "
                      f"Performance {'âœ…' if perf_test['summary']['overall_pass'] else 'âŒ'}")

                # Wait for next minute (minus measurement time)
                if minute < duration_minutes - 1:
                    sleep_time = 60 - (time.time() - measurement_time)
                    if sleep_time > 0:
                        time.sleep(sleep_time)

        except KeyboardInterrupt:
            print("\nâ¹ï¸  Monitoring session interrupted")
        finally:
            self.performance_monitor.stop_monitoring()

        # Generate session summary
        session_end = time.time()
        session_data.update({
            "session_end": datetime.now().isoformat(),
            "actual_duration_seconds": session_end - session_start,
            "total_measurements": len(session_data["measurements"]),
            "session_summary": self._analyze_monitoring_session(session_data)
        })

        return session_data

    def _analyze_monitoring_session(self, session_data: Dict) -> Dict[str, Any]:
        """Analyze monitoring session data for trends and insights"""
        measurements = session_data.get("measurements", [])
        if not measurements:
            return {"error": "No measurements to analyze"}

        # Extract performance test results
        test_results = [m["performance_test"] for m in measurements]
        success_rates = [t["summary"]["success_rate_percent"] for t in test_results]
        test_durations = [t["total_test_duration_ms"] for t in test_results]

        # Calculate trends
        avg_success_rate = sum(success_rates) / len(success_rates)
        avg_test_duration = sum(test_durations) / len(test_durations)

        # Stability analysis
        success_rate_std = (sum((x - avg_success_rate) ** 2 for x in success_rates) / len(success_rates)) ** 0.5
        duration_std = (sum((x - avg_test_duration) ** 2 for x in test_durations) / len(test_durations)) ** 0.5

        return {
            "measurements_analyzed": len(measurements),
            "performance_stability": {
                "avg_success_rate_percent": avg_success_rate,
                "success_rate_stability": "stable" if success_rate_std < 5 else "variable",
                "avg_test_duration_ms": avg_test_duration,
                "duration_stability": "stable" if duration_std < 2 else "variable"
            },
            "optimizer_validation": {
                "all_tests_passed": all(t["summary"]["overall_pass"] for t in test_results),
                "consistency": "consistent" if success_rate_std < 2 else "inconsistent",
                "target_compliance": avg_test_duration <= 15  # Allow some overhead for full test
            },
            "recommendations": [
                f"Average success rate: {avg_success_rate:.1f}% ({'within target' if avg_success_rate >= 95 else 'below target'})",
                f"Performance consistency: {duration_std:.2f}ms standard deviation",
                "Continue monitoring for long-term trend analysis"
            ]
        }


def main():
    """CLI interface for monitoring dashboard"""
    import argparse

    parser = argparse.ArgumentParser(description='ARTIFACTOR Monitoring Dashboard')
    parser.add_argument('--dashboard', action='store_true',
                       help='Start real-time dashboard')
    parser.add_argument('--interval', type=float, default=5.0,
                       help='Dashboard refresh interval in seconds')
    parser.add_argument('--session', type=int, default=0,
                       help='Run monitoring session for N minutes')
    parser.add_argument('--report', action='store_true',
                       help='Generate comprehensive monitoring report')
    parser.add_argument('--export', type=str,
                       help='Export results to JSON file')

    args = parser.parse_args()

    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(levelname)s | %(name)s | %(message)s'
    )

    dashboard = MonitoringDashboard()

    try:
        if args.dashboard:
            dashboard.start_dashboard(refresh_interval=args.interval)

        elif args.session:
            session_data = dashboard.run_comprehensive_monitoring_session(duration_minutes=args.session)
            if args.export:
                with open(args.export, 'w') as f:
                    json.dump(session_data, f, indent=2)
                print(f"ğŸ“ Session data exported to {args.export}")
            else:
                print(json.dumps(session_data, indent=2))

        elif args.report:
            report = dashboard.generate_monitoring_report()
            if args.export:
                with open(args.export, 'w') as f:
                    json.dump(report, f, indent=2)
                print(f"ğŸ“ Report exported to {args.export}")
            else:
                print(json.dumps(report, indent=2))

        else:
            # Quick status check
            print("ğŸ“Š ARTIFACTOR v2.0 Quick Status")
            print("=" * 40)

            # Quick performance test
            monitor = PerformanceMonitor()
            test_results = monitor.run_performance_regression_test()
            summary = test_results['summary']

            print(f"âœ… Performance Test: {summary['success_rate_percent']:.0f}% pass rate")
            print(f"â±ï¸  Test Duration: {test_results['total_test_duration_ms']:.1f}ms")
            print(f"ğŸ¯ OPTIMIZER Targets: {'âœ… Met' if summary['overall_pass'] else 'âŒ Not Met'}")

            print("\nğŸ’¡ Use --dashboard for real-time monitoring")
            print("ğŸ’¡ Use --report for comprehensive analysis")

    except Exception as e:
        print(f"âŒ Error: {e}")
        return 1

    return 0


if __name__ == '__main__':
    exit(main())